---
layout: post
title: "Applicaion data pipeline - навчальний проект"
date: 2025-12-16 10:00:01
categories: [azure, yolo8, open-cv, python, azure durable function, az-204]
permalink: posts/2025-12-16/az-datapipeline-ua/
published: true
---

<!-- TOC BEGIN -->

- [1. Про що цей блог](#p-1")
- [2. Edge Layer (Межа): Розумна детекція на Raspberry Pi 5](#p-2)
- [3. Обробка відео-файлі в хмарі](#p-3)
- [3.1. Реакція хмари на завантажений файл](#p-3.1)



<!-- TOC END -->

## <a name="p-1">1. Про що цей блог</a>

Це продовження блогів про сервіси azure:
- [AZURE AZ-204 AZ BLOB STORAGE - навчальний проект](https://pavlo-shcherbukha.github.io/posts/2025-10-16/az-204-blob-strg-ua/);
- [AZURE AZ-204 FOR DEVELOPERS](https://pavlo-shcherbukha.github.io/posts/2024-01-30/az-204-dev/),

що проходять по курсу az-204 шукав задачу для прототипа, що дозволить використати azure durable function. Фактично, мені потрібна була ідея для якогось важкого обчислювального процесу. Найбільш очевидною ідеєю, що прийшла в голову, це підключити камеру і обробити відео файл. Іншими словами почати використовувавти елементи computer vision. Назвав я цю ідею: **"Відеоспостереження на дачі"**.  
**Суть ідеї** полягає  в тому, щоб камеру чи камери підключити на дачі до raspberry PY. На Raspberry  налаштувати детекцію руху. У випадку детекції писати 30 секунд відео. Потім записане відео завантажується в хмару azure  для подальшого аналізу вже "важкими" інстументами, а саме: пропустити відео через нейромережу для детекції об'єктів, що є на  кадрах. А всі детектовані кадри зберегти в окремому розділі, що пов'заний з цим відео.
Якщо подивитися зі сторони оцінки вкладеної праці і часу, то ідея надто складна. Набагато простіше купити роутер, tplink tapo  камеру. Підключити камеру до мобільного додатку і якоїсь китайської хмари і за умовних 200 гривень у вас працює відеоспостереження.

Але якщо ціль в тому щоб:

- навчитися проектувати і розробляти розподілені додатки в хмарі,
- підготувати шаблон надійного прикладного data pipeline  для роботи зі штучним інтелектом,
- підготувати шаблон  архітектури для подальшого розвитку типу "розумний будинок" чи "цифрові двійники" -  то на мій особитсий погляд, то така робота має сенс.

Узагальнена архітектура додатку показана на [pic-01](#pic-01)

<kbd><img src="../assets/img/posts/2025-12-16-az-datapipeline/doc/pic-01.png" /></kbd>
<p style="text-align: center;"><a name="pic-01">pic-01</a></p>





## <a name="p-2">2. Edge Layer (Межа): Розумна детекція на Raspberry Pi 5</a>

Для інтеграції камери та  edge обчислювального пристрою вибрав rtsp протокол. Це досить не залежний протокол, через який можна підключити камеру, чи мобільний телефон з додатком wifi-камера (ip-камера). Цей протокол не дає використати всі можливості камер якогось виробника. Але мені не вдалося знайти відкритих бібліотек по управлінню тими ж камерами tp-link tapo. Тому я викорастав той протокл, що добре відомий і до якого є бібліотеки.
В якості Edge комп'ютера використав Raspberry PI 5 з 8 мегабайтами пам'яті. Для обробки трафіку від камери на Raspberry PI написана програма на Python з використанням бібліотеки open-cv.  Для швидкої обробки кадрів відео Raspberry PI 5  підходить ідеально. Ну і для підтримки роботи ком'ютера при відключеннях світла використав ДБЖ 5V/5A UPS Module 3S.
Детекція руху виконується програмно за допомогою бібліотеки open-cv. Якщо рух виявлено, то виконується запис відео на диск протягом 30 секунд. Коли  запис закінчено, файл з відео завантажено в хмару azure на мій BlobStorage. На цьому, робота Edge ком'ютера завершується, а подальша обробка відео виконується вже у хмарі.
Так як Raspberry 5  має багато зовнішніх портів - функціональність Edge комп'ютера можна легко розширити підключивши датчики чи якісь інші прилади, для  отримання даних та передачу їх в хмару.


## <a name="p-3">3. Обробка відео-файлі в хмарі</a>

### <a name="p-3.1">3.1. Реакція хмари на завантажений файл</a>

2. Triggering (Подія): Миттєва реакція хмари

    Як тільки завантаження файлу завершено, Azure Event Grid автоматично «підхоплює» цю подію.

    Він відправляє сигнал (HTTP trigger) до нашого оркестратора — Azure Durable Function.

3. Orchestration (Диригент): Durable Functions

    Це серце системи. Функція не просто виконує код, вона керує складним процесом:

        Вона запускає Azure Container Instance (ACI).

        Вона передає параметри (URL відео) у контейнер.

        Вона переходить у стан «очікування», не споживаючи ресурси (і ваші гроші), поки контейнер працює.

4. Heavy Lifting (Обробка): AI в контейнерах

    Ми використовуємо Azure Container Instances, бо вони дозволяють запустити важку модель (наприклад, YOLOv8) для розпізнавання об'єктів.

    Контейнер завантажує відео, аналізує його (хто в кадрі: людина, машина чи просто собака?) і записує результат назад у сховище або чергу.

5. Feedback Loop: Результат та сповіщення

    Після завершення обробки контейнер надсилає сигнал оркестратору.

    Система приймає рішення: якщо знайдено людину — відправити push-сповіщення, якщо просто тінь — позначити файл як «безпечний».

Порада для блогу:

Додайте в опис коротку таблицю "Вартість рішення". Для читачів DIY-проєктів це завжди найцікавіше:

    Raspberry Pi: Разова інвестиція.

    Azure Blob/Functions: Майже безкоштовно на малих об'ємах.

    Azure Container Instances: Оплата лише за секунди роботи (наприклад, 2-3 центи за один запуск аналізу).

Це покаже, що ваша архітектура не тільки технологічна, а й економічно виправдана.

Ви вже визначилися, яку саме модель розпізнавання об'єктів (YOLO, TensorFlow Lite тощо) будете "пакувати" у контейнер? Можу підказати, як підготувати оптимальний Docker-образ для Azure.
